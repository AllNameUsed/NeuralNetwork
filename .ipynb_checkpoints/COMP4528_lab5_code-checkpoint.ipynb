{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMP4528 Lab4\n",
    "\n",
    "In this lab, we'll delve into the implementation of deep neural networks for computer vision classification using PyTorch. It's essential to have a solid understanding of PyTorch programming for Assignment 2.\n",
    "\n",
    "In this lab, we will cover:\n",
    "- Implement PyTorch Dataset and DataLoader classes\n",
    "- Implement a simple fully-connected network\n",
    "- Implement a convolutional neural network\n",
    "- Implement the training and testing function for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['arr_0']\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "betas = (0.9, 0.999)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "# make results determinstic\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "labels = np.load(r\"data/kmnist-test-labels.npz\").files\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class KMNIST(Dataset):\n",
    "    # TODO: Implement the KMNIST Dataset class according to the PyTorch tutorial\n",
    "    #       https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "    def __init__(self, img_path, label_path, transforms):\n",
    "        self.labels = np.load(label_path)['arr_0']\n",
    "        self.imgs = np.load(img_path)['arr_0']\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: transformation defined for this classification task, data augmentation techniques used during training.\n",
    "#       To understand each transformation, please read https://pytorch.org/vision/stable/transforms.html\n",
    "#       - (Normalize data to range between [-1, 1])\n",
    "#       - (Randomly flip the image left to right)\n",
    "#       - (Zero-pad 4 pixels on each side and randomly crop 28x28 as input)\n",
    "train_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.ConvertImageDtype(torch.float),\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "    T.RandomCrop(size=(28, 28), padding=4),\n",
    "])\n",
    "\n",
    "test_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.ConvertImageDtype(torch.float),\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load training, validation and testing data use the Dataset and transformations defined above.\n",
    "#       Then, for each dataset, create the corresponding DataLoader class with batch_size provided\n",
    "#       https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders\n",
    "train_dataset = KMNIST(\n",
    "    \"data/kmnist-train-imgs.npz\",\n",
    "    \"data/kmnist-train-labels.npz\",\n",
    "    train_transforms,\n",
    ")\n",
    "val_dataset = KMNIST(\n",
    "    \"data/kmnist-val-imgs.npz\",\n",
    "    \"data/kmnist-val-labels.npz\",\n",
    "    test_transforms,\n",
    ")\n",
    "test_dataset = KMNIST(\n",
    "    \"data/kmnist-test-imgs.npz\",\n",
    "    \"data/kmnist-test-labels.npz\",\n",
    "    test_transforms,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0)\n",
    "val_loader = DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0)\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcClassifier(nn.Module):\n",
    "    # TODO: Implement a fully-connected neural network with an architecture listed below\n",
    "    #       - Fully-connected layer with 512 output units.\n",
    "    #       - ReLU Activation Layer.\n",
    "    #       - Fully-connected layer with 128 output units.\n",
    "    #       - ReLU Activation Layer.\n",
    "    #       - Fully-connected layer with 10 output units.\n",
    "    #       https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "    def __init__(self):\n",
    "        super(FcClassifier, self).__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        # Third fully connected layer that outputs our 10 labels\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # NOTE: Flatten has been done\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # output = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, device, optimizer, n_epoch):\n",
    "    # TODO: Implement training code, you should `print out` training and validation accuracy and loss during training\n",
    "    #       You should define the loss function (cross-entropy loss) within this function\n",
    "    #       Remember to load the best state_dict (highest validation accuracy) before exiting this function, so you are not overfitting.\n",
    "    #       https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    max_acc = 0\n",
    "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device) \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch number \" + str(epoch)+\" , loss = \" + str(loss.item()))\n",
    "        acc, l = test(model, val_loader, device)\n",
    "        print(\"Evaluation accuracy \" + str(acc)+\" , loss = \" + str(l))\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    # TODO: Implement testing code, you should `RETURN` the testing accuracy and loss on the given test_loader\n",
    "    #       You should define the loss function (cross-entropy loss) within this function\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device) \n",
    "            labels = labels.to(device) \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total \n",
    "    return accuracy, loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FcClassifier().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 , loss = 1.1153783798217773\n",
      "Evaluation accuracy 68.2 , loss = 0.5926669239997864\n",
      "Epoch number 1 , loss = 0.7342511415481567\n",
      "Evaluation accuracy 70.2 , loss = 0.9192866086959839\n",
      "Epoch number 2 , loss = 0.8607487678527832\n",
      "Evaluation accuracy 69.8 , loss = 1.0775744915008545\n",
      "Epoch number 3 , loss = 0.9003502726554871\n",
      "Evaluation accuracy 70.7 , loss = 0.748070240020752\n",
      "Epoch number 4 , loss = 0.5626799464225769\n",
      "Evaluation accuracy 70.8 , loss = 0.7226117849349976\n",
      "Epoch number 5 , loss = 0.8733702301979065\n",
      "Evaluation accuracy 71.1 , loss = 0.7504605650901794\n",
      "Epoch number 6 , loss = 0.8372029662132263\n",
      "Evaluation accuracy 70.1 , loss = 0.8910858035087585\n",
      "Epoch number 7 , loss = 0.5554274916648865\n",
      "Evaluation accuracy 72.8 , loss = 0.6326558589935303\n",
      "Epoch number 8 , loss = 0.7837399840354919\n",
      "Evaluation accuracy 74.0 , loss = 1.055847406387329\n",
      "Epoch number 9 , loss = 0.5032846331596375\n",
      "Evaluation accuracy 74.8 , loss = 0.8207271695137024\n",
      "Epoch number 10 , loss = 0.5611507296562195\n",
      "Evaluation accuracy 74.5 , loss = 0.9005026817321777\n",
      "Epoch number 11 , loss = 0.7136766314506531\n",
      "Evaluation accuracy 75.3 , loss = 0.5232817530632019\n",
      "Epoch number 12 , loss = 0.7553868889808655\n",
      "Evaluation accuracy 75.4 , loss = 0.8253293037414551\n",
      "Epoch number 13 , loss = 0.4904612898826599\n",
      "Evaluation accuracy 74.4 , loss = 0.5084935426712036\n",
      "Epoch number 14 , loss = 0.6454137563705444\n",
      "Evaluation accuracy 75.3 , loss = 0.7403463125228882\n",
      "Epoch number 15 , loss = 0.8261038661003113\n",
      "Evaluation accuracy 75.4 , loss = 0.7678357362747192\n",
      "Epoch number 16 , loss = 0.9156827330589294\n",
      "Evaluation accuracy 75.8 , loss = 0.5716029405593872\n",
      "Epoch number 17 , loss = 0.5073739290237427\n",
      "Evaluation accuracy 76.3 , loss = 0.5484288930892944\n",
      "Epoch number 18 , loss = 0.4455612003803253\n",
      "Evaluation accuracy 76.0 , loss = 0.5914171934127808\n",
      "Epoch number 19 , loss = 0.6899401545524597\n",
      "Evaluation accuracy 75.6 , loss = 0.6515113115310669\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FcClassifier(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, device, optimizer, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.605023205280304, Test accuracy: 75.6\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test(model, test_loader, device)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 , loss = 0.9550157785415649\n",
      "Evaluation accuracy 63.8 , loss = 1.1631648540496826\n",
      "Epoch number 1 , loss = 0.8217390775680542\n",
      "Evaluation accuracy 63.8 , loss = 0.8047768473625183\n",
      "Epoch number 2 , loss = 0.78706294298172\n",
      "Evaluation accuracy 65.1 , loss = 0.8471577763557434\n",
      "Epoch number 3 , loss = 1.105034589767456\n",
      "Evaluation accuracy 66.3 , loss = 0.5618926286697388\n",
      "Epoch number 4 , loss = 0.7950542569160461\n",
      "Evaluation accuracy 66.3 , loss = 0.8734108805656433\n",
      "Epoch number 5 , loss = 0.64277184009552\n",
      "Evaluation accuracy 66.1 , loss = 1.0858489274978638\n",
      "Epoch number 6 , loss = 0.6162272691726685\n",
      "Evaluation accuracy 65.4 , loss = 0.7936688661575317\n",
      "Epoch number 7 , loss = 0.9403723478317261\n",
      "Evaluation accuracy 66.7 , loss = 1.3389756679534912\n",
      "Epoch number 8 , loss = 0.42294079065322876\n",
      "Evaluation accuracy 66.9 , loss = 0.5504032969474792\n",
      "Epoch number 9 , loss = 1.068264126777649\n",
      "Evaluation accuracy 65.8 , loss = 0.8499429821968079\n",
      "Epoch number 10 , loss = 0.7901517152786255\n",
      "Evaluation accuracy 75.1 , loss = 1.2492454051971436\n",
      "Epoch number 11 , loss = 0.6756723523139954\n",
      "Evaluation accuracy 75.8 , loss = 0.4984496235847473\n",
      "Epoch number 12 , loss = 0.4609222710132599\n",
      "Evaluation accuracy 82.5 , loss = 0.517025351524353\n",
      "Epoch number 13 , loss = 0.5250621438026428\n",
      "Evaluation accuracy 82.2 , loss = 0.7751191854476929\n",
      "Epoch number 14 , loss = 0.29344412684440613\n",
      "Evaluation accuracy 83.2 , loss = 0.7694649696350098\n",
      "Epoch number 15 , loss = 0.2504642903804779\n",
      "Evaluation accuracy 82.9 , loss = 0.5586467385292053\n",
      "Epoch number 16 , loss = 0.5622568726539612\n",
      "Evaluation accuracy 83.0 , loss = 0.45873910188674927\n",
      "Epoch number 17 , loss = 0.5745886564254761\n",
      "Evaluation accuracy 83.6 , loss = 0.405573308467865\n",
      "Epoch number 18 , loss = 0.7424484491348267\n",
      "Evaluation accuracy 83.4 , loss = 0.27900251746177673\n",
      "Epoch number 19 , loss = 0.5796812772750854\n",
      "Evaluation accuracy 82.0 , loss = 0.5726265907287598\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FcClassifier(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, device, optimizer, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4096739888191223, Test accuracy: 83.9\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test(model, test_loader, device)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    # TODO: Implement the convolutional neural network with the following architecture\n",
    "    #       - 5×5 Convolutional Layer with 32 filters, stride 1 and padding 2.\n",
    "    #       - ReLU Activation Layer.\n",
    "    #       - 2×2 Max Pooling Layer with a stride of 2.\n",
    "    #       - 3×3 Convolutional Layer with 64 filters, stride 1 and padding 1.\n",
    "    #       - ReLU Activation Layer.\n",
    "    #       - 2×2 Max Pooling Layer with a stride of 2。\n",
    "    #       - Fully-connected layer with 1024 output units.\n",
    "    #       - ReLU Activation Layer.\n",
    "    #       - Fully-connected layer with 10 output units.\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.subsamping = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.subsamping(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.subsamping(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 , loss = 0.17248190939426422\n",
      "Evaluation accuracy 93.1 , loss = 0.062367163598537445\n",
      "Epoch number 1 , loss = 0.06702597439289093\n",
      "Evaluation accuracy 95.6 , loss = 0.18980515003204346\n",
      "Epoch number 2 , loss = 0.053638994693756104\n",
      "Evaluation accuracy 96.7 , loss = 0.04805807024240494\n",
      "Epoch number 3 , loss = 0.09582117944955826\n",
      "Evaluation accuracy 97.6 , loss = 0.02678241766989231\n",
      "Epoch number 4 , loss = 0.006938578095287085\n",
      "Evaluation accuracy 96.6 , loss = 0.11795906722545624\n",
      "Epoch number 5 , loss = 0.019981175661087036\n",
      "Evaluation accuracy 98.2 , loss = 0.08908823132514954\n",
      "Epoch number 6 , loss = 0.009502657689154148\n",
      "Evaluation accuracy 98.1 , loss = 0.05021745711565018\n",
      "Epoch number 7 , loss = 0.0218160692602396\n",
      "Evaluation accuracy 97.7 , loss = 0.041722867637872696\n",
      "Epoch number 8 , loss = 0.06459479033946991\n",
      "Evaluation accuracy 97.5 , loss = 0.012130286544561386\n",
      "Epoch number 9 , loss = 0.0718238577246666\n",
      "Evaluation accuracy 98.0 , loss = 0.06090028956532478\n",
      "Epoch number 10 , loss = 0.02025928907096386\n",
      "Evaluation accuracy 97.3 , loss = 0.001418111496604979\n",
      "Epoch number 11 , loss = 0.0015871988143771887\n",
      "Evaluation accuracy 98.1 , loss = 0.036408718675374985\n",
      "Epoch number 12 , loss = 0.025816941633820534\n",
      "Evaluation accuracy 98.0 , loss = 0.001229574205353856\n",
      "Epoch number 13 , loss = 0.047786910086870193\n",
      "Evaluation accuracy 98.4 , loss = 0.0410287007689476\n",
      "Epoch number 14 , loss = 0.04974878579378128\n",
      "Evaluation accuracy 98.1 , loss = 0.4191851019859314\n",
      "Epoch number 15 , loss = 0.012946057133376598\n",
      "Evaluation accuracy 98.2 , loss = 0.07574794441461563\n",
      "Epoch number 16 , loss = 0.00817078910768032\n",
      "Evaluation accuracy 98.9 , loss = 0.0975506454706192\n",
      "Epoch number 17 , loss = 0.018017413094639778\n",
      "Evaluation accuracy 98.5 , loss = 0.009464764967560768\n",
      "Epoch number 18 , loss = 0.0032930674497038126\n",
      "Evaluation accuracy 98.5 , loss = 0.03506329283118248\n",
      "Epoch number 19 , loss = 0.002477426780387759\n",
      "Evaluation accuracy 98.3 , loss = 0.006697692908346653\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (subsamping): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, device, optimizer, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.06300608813762665, Test accuracy: 98.8\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test(model, test_loader, device)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: れ\n",
      "Ground truth: れ\n"
     ]
    }
   ],
   "source": [
    "# Visualization and predictions\n",
    "classes = ('お', 'き', 'す', 'つ', 'な', 'は', 'ま', 'や', 'れ', 'を')\n",
    "img, label = test_dataset[6]\n",
    "plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "plt.show()\n",
    "img = img.to(device)\n",
    "pred = int(torch.argmax(model(img.unsqueeze(0)), dim=1)[0])\n",
    "print(f\"Model prediction: {classes[label]}\\nGround truth: {classes[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a deeper CNN model for a higher classification accuracy\n",
    "**NOTE: This part would require GPU computing power. You may skip this part during the lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    # TODO: Implement the convolutional neural network with more layers than the previous one.\n",
    "    #       Test the model accuracy and see whether there is a performance gain.\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # self.subsamping = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 , loss = 0.07278585433959961\n",
      "Evaluation accuracy 97.5 , loss = 0.20767100155353546\n",
      "Epoch number 1 , loss = 0.061321910470724106\n",
      "Evaluation accuracy 98.2 , loss = 0.0014424609253183007\n",
      "Epoch number 2 , loss = 0.02370794117450714\n",
      "Evaluation accuracy 97.6 , loss = 0.11627461016178131\n",
      "Epoch number 3 , loss = 0.003226394299417734\n",
      "Evaluation accuracy 98.1 , loss = 0.16836097836494446\n",
      "Epoch number 4 , loss = 5.010820586903719e-06\n",
      "Evaluation accuracy 98.1 , loss = 0.12929776310920715\n",
      "Epoch number 5 , loss = 0.003897326299920678\n",
      "Evaluation accuracy 98.4 , loss = 0.6596704721450806\n",
      "Epoch number 6 , loss = 0.03734453395009041\n",
      "Evaluation accuracy 98.9 , loss = 0.03715340048074722\n",
      "Epoch number 7 , loss = 0.21569590270519257\n",
      "Evaluation accuracy 98.2 , loss = 1.5775141716003418\n",
      "Epoch number 8 , loss = 0.02401292696595192\n",
      "Evaluation accuracy 98.2 , loss = 0.12760813534259796\n",
      "Epoch number 9 , loss = 0.02663019672036171\n",
      "Evaluation accuracy 98.7 , loss = 0.01485955249518156\n",
      "Epoch number 10 , loss = 0.03840959072113037\n",
      "Evaluation accuracy 97.9 , loss = 0.13303926587104797\n",
      "Epoch number 11 , loss = 0.05781076103448868\n",
      "Evaluation accuracy 98.2 , loss = 0.00030839774990454316\n",
      "Epoch number 12 , loss = 0.019872790202498436\n",
      "Evaluation accuracy 97.9 , loss = 0.22885997593402863\n",
      "Epoch number 13 , loss = 2.8362397642922588e-05\n",
      "Evaluation accuracy 97.7 , loss = 3.0120490919216536e-05\n",
      "Epoch number 14 , loss = 0.04584368318319321\n",
      "Evaluation accuracy 98.9 , loss = 0.008705354295670986\n",
      "Epoch number 15 , loss = 0.013364559039473534\n",
      "Evaluation accuracy 98.0 , loss = 0.002405588049441576\n",
      "Epoch number 16 , loss = 0.005227581597864628\n",
      "Evaluation accuracy 98.0 , loss = 0.22514745593070984\n",
      "Epoch number 17 , loss = 0.01388377882540226\n",
      "Evaluation accuracy 98.2 , loss = 1.4222334623336792\n",
      "Epoch number 18 , loss = 0.04330824688076973\n",
      "Evaluation accuracy 97.6 , loss = 0.0005132464575581253\n",
      "Epoch number 19 , loss = 0.00012814518413506448\n",
      "Evaluation accuracy 98.6 , loss = 9.499384759692475e-06\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=3136, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, device, optimizer, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1373709738254547, Test accuracy: 98.1\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test(model, test_loader, device)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "16fb029d2d2f281c76137439d9122480f0701165270269ef0c874281e28a4d48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
